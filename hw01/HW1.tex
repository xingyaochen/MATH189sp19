\documentclass[12pt,letterpaper]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{enumitem}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SU17}
\assignment{Homework 1}
\duedate{Tuesday, May 15, 2017}

\renewcommand{\labelenumi}{{(\alph{enumi})}}


\begin{document}
Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
The starter code for problem 2 part c and d can be found under the Resource tab on course website.\\

\textit{Note:} You need to create a Github account for submission of the coding part of the homework. Please create a repository on Github to hold all your code and include your Github account username as part of the answer to problem 2.

\begin{problem}[1]
(\textbf{Linear Transformation}) Let $\mathbf{y} = A\mathbf{x} + \mathbf{b}$ be a random vector.
show that expectation is linear:
\[
    \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb.
\]
Also show that
\[
    \cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.
\]
\end{problem}
\begin{solution}
Part 1:\\
Since $\EE$ is a weighted sum of all random variable, $\EE$ is a linear operator.
First prove that
\[\EE(aX + b) = a\EE(X)+ b\]
for a random variable.\\
If $X$ is a random variables, and
$a$ and $b$ are constants, \\
Case 1: if $X$ is discrete:
\begin{align*}
  \EE[aX + b] =& \sum_{x \in X} (ax+b)p(x)\\
  =&a\sum_{x\in X} xp(x)+ \sum_{x\in X} bp(x)\\
  =& a\EE[X] + b
\end{align*}
\noindent
Case 2: if $X$ is continuous:
\begin{align*}
  \EE[aX + b] =& \int_{\mathbb{R}} (ax+b)p(x) dx\\
  =& a\int_{\mathbb{R}}xp(x)dx + b \int p(x) dx\\
  =& a\EE[X] + b
\end{align*}


Now, we show that this is true for a random vector:
 Let $A$ be a constant $m\times n$ matrix,
 $\xx$ be a length $n$ random vetor,
 and $\bb$ be a length $m$ constant vector.
 Consider a single element in random vector $\yy$:
    \begin{align*}
      \EE[y_i]=&
    \EE\left[\sum_{j=1}^n a_{i, j} x_j +b_i\right]\\
    =&\sum_{j=1}^n a_{i, j} \EE[x_j] + b_i\\
    \end{align*}
    Therefore we see that
    \[    \EE[\yy] = \EE[A\xx +\bb] = A\EE[\xx ]+ \bb \]
    \qed

\noindent Part 2: \\
The prove this property, we first show that $\cov[\zz + \bb] = \cov[\zz]$, then we show that $\cov[A\xx ] = A\cov[\xx]A^T$. \\
For $\cov[\zz + \bb] = \cov[\zz]$, let $\zz$ be a length $n$ random vector and $\bb$ be a constant $n$-length vector.
Let $\Sigma = \cov[\zz + \bb]$.
\begin{align*}
  \Sigma_{i,j} =& \cov[z_i + b_i, z_j+b_j]\\
  =& \EE \left[ ( (z_i+a_i)-\EE[z_i+a_i])((z_j+a_j)-\EE[z_j+a_j])) \right]\\
  =& \EE \left[ ( z_i+a_i-a_i-\EE[z_i])(z_j+a_j-a_j-\EE[z_j])) \right]\\
  =& \EE \left[ ( z_i-\EE[x_i])(z_j-\EE[z_j])) \right]\\
  =&\cov[z_i + b_i, z_j+b_j]\\
\end{align*}
Therefore, we see that $\cov[\zz + \bb] = \cov[\zz]$. \\
For $\cov[A\xx ] = A\cov[\xx]A^T$, let $A$ be a $m \times n$ matrix and let $\xx$
be a length $n$ vector.
Let $\Sigma' = \cov[A\xx]$. Consider element $(i, j)$ in $\Sigma'$.

\begin{align*}
\Sigma'_{i, j} =& \cov\left( \sum_{k=1}^n a_{i, k}x_k, \sum_{k=1}^n a_{j, k}x_k\right]\\
=& \EE\left[ \left(\sum_{k=1}^n a_{i, k}x_k - \EE\left[\sum_{k=1}^n a_{i, k}x_k\right]\right) \left(\sum_{k=1}^n a_{j, k}x_k - \EE\left[\sum_{k=1}^n a_{j, k}x_k\right]\right)  \right]\\
=& \EE\left[ \left(\sum_{k=1}^n a_{i, k}x_k - \sum_{k=1}^n a_{i, k} \EE[x_k]\right) \left(\sum_{k=1}^n a_{j, k}x_k - \sum_{k=1}^n a_{j, k}\EE [x_k]\right)  \right]\\
=& \EE\left[ \sum_{k=1}^n a_{i, k} \left(x_k - \EE[x_k]\right) \left(x_k - \EE [x_k]\right)\sum_{k=1}^n a_{j, k}  \right]\\
=&  \sum_{k=1}^n a_{i, k} \left( \EE\left[\left(x_k - \EE[x_k]\right) \left(x_k - \EE [x_k]\right) \right]\right) \sum_{k=1}^n a_{j, k}  \\
\end{align*}

From this, we see that $\Sigma' = A\cov[\xx] A^T$,
If we let $\cov[\xx] = \Sigma$, then
 $\Sigma' = A\Sigma A^T$. \\

Since we have proven both of these properties,
we can simply substitute $\zz$ for $A\xx$:
\begin{align*}
  \cov[A\xx + \bb] =&\cov[\zz + \bb] \\
  =&\cov[\zz ] \\
  =& \cov[A\xx] \\
  =& A\Sigma A^T
\end{align*}
\qed

\end{solution}


\newpage




\begin{problem}[2]
Given the dataset $\Dc = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
\begin{enumerate}
   \item Find the least squares estimate $y = \thetab^\T\xx$ by hand using
        Cramer's Rule.
    \item Use the normal equations to find the same solution and verify it
        is the same as part (a).
    \item Plot the data and the optimal linear fit you found.
    \item Find randomly generate 100 points near the line with white Gaussian
        noise and then compute the least squares estimate (using a computer).
        Verify that this new line is close to the original and plot the new
        dataset, the old line, and the new line.
\end{enumerate}

\end{problem}
\begin{solution}
    \begin{enumerate}
      \item
      \begin{tabular}{l|l|| l| l }
        $x_i$ & $y_i$ & $x_i^2$ & $x_iy_i$\\
        \hline
        0 & 1 & 0 & 0\\
        2 & 3 & 4 & 6 \\
        3 & 6 & 9 & 18 \\
        4 & 8 & 16 & 32
      \end{tabular}
      \begin{align*}
        m =& \frac{n \sum_{i=1}^n x_iy_i  - \left(\sum_{i=1}^n x_i\right) \left(\sum_{i=1}^n y_i\right) }{ \left(n\sum_{i=1}^n x_i^2 \right) - \left(\sum_{i=1}^n x_i \right)^2}\\
        =& \frac{4\cdot 56 - 9\cdot 18}{ 4 \cdot 29 -  81}\\
        \approx & 1.77\\
        b =& \frac{\left(\sum_{i=1}^n x_i^2\right) \left( \sum_{i=1}^n  y_i\right)
        - \left(\sum_{i=1}^n x_i\right)\left(\sum_{i=1}^n x_iy_i\right)}{
        n\left(\sum_{i=1}^n x_i^2\right)- \left(\sum_{i=1}^n x_i\right)^2}\\
        =& \frac{ 29\cdot 19 - 9\cdot 56 }{4\cdot 29 - 81}\\
        =& -6.94
      \end{align*}

    \item \begin{align*}
    \theta =& (X^TX)^{-1}X^T\yy\\
    =& 29^{-1}\begin{bmatrix}
    0&2&3 &4
  \end{bmatrix}\begin{bmatrix}
  1 \\3\\6\\8
  \end{bmatrix}\\
  \approx & 1.93
    \end{align*}
    \end{enumerate}
\end{solution}
\newpage



\end{document}
